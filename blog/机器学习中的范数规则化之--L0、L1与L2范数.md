# 机器学习中的范数规则化之 —— L0、L1与L2范数

> 内容转载自[http://blog.csdn.net/zouxy09](http://blog.csdn.net/zouxy09/article/details/24971995)，仅供自己学习使用，如果侵权，请联系我 736553312@qq.com ,第一时间删除。

监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。

 一般来说，监督学习可以看做最小化下面的目标函数：
 ```math
 w^*=arg\ min_w\ \sum\ L(y_i,f(x_i;w))+\lambda\Omega(w)
 ```
 ![image](http://img.blog.csdn.net/20140504122253546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
 
 其中，第一项L(yi,f(xi;w))衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小(防止过拟合)，所以我们需要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。
 
 OK，到这里，如果你在机器学习浴血奋战多年，你会发现，哎哟哟，机器学习的大部分带参模型都和这个不但形似，而且神似。是的，**其实大部分无非就是变换这两项而已**。对于第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是牛逼的 Boosting了；如果是log-Loss，那就是Logistic Regression了；还有等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。但这里，我们先不究loss函数的问题，我们把目光转向“规则项Ω(w)”。
 
 规则化函数Ω(w)也有很多种选择，一般是++模型复杂度的单调递增函数++，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同，但我们在论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等。
 
>L0范数是指向量中非0的元素的个数

>L1范数是指向量中各个元素绝对值之和

>L2范数是指向量各元素的平方和然后求平方根,通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。

其他信息参考原文